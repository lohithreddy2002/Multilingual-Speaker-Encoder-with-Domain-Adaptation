training: !!bool "true"
device: "cpu"
datasets:
    VCTK:
        root: '~/Downloads/VCTK-Corpus'
        test_speakers: ['p226', 'p241', 'p245', 'p265', 'p280', 'p303', 'p311', 'p339']
    JVS:
        root: '~/Downloads/jvs_ver1'
        test_speakers: ['jvs001', 'jvs014', 'jvs021', 'jvs036', 'jvs071', 'jvs084', 'jvs092', 'jvs098']
    TEL:
        root: "/home/lohith/telugu_dataset"
        test_speakers: ['08318', '04830', '09935','09015', '09584', '06885']
train_datasets: ["TEL"]
test_speaker_per_dataset: 8

---
data:
    train_path: './train_tisv'
    test_path: './test_tisv'
    data_preprocessed: !!bool "true" 
    sr: 16000
    nfft: 1024 # For mel spectrogram preprocess
    window: 0.05 # (s)
    hop: 0.0125 # (s)   
    nmels: 80 #Number of mel energies
    tisv_frame: 150 #Max number of time steps in input after preprocess
    fmin: 55
    fmax: 8000
---   
model:
    hidden: 256 #Number of LSTM hidden layer units
    num_layer: 3 #Number of LSTM layers
    proj: 64 #Embedding size
    model_name: 'baseline' #Model path for testing, inference, or resuming training
    loss: 'softmax'
    da: false
    da_on: 'language'
    da_startpoint: 0
    lang_emb_dim: 32
    architecture: 'LSTM'
---
train:
    N : 4 #Number of speakers in batch
    M : 10 #Number of utterances per speaker
    num_workers: 4 #number of workers for dataloader
    lr: 0.001 
    epochs: 2400 #Max training speaker epoch 
    log_interval: 5 #Epochs before printing progress
    log_file: './log.txt'
    checkpoint_interval: 200 #Save model after x speaker epochs
    checkpoint_dir: './output'
    restore: true #Resume training from previous model path
    start_epoch: 2000
    anneal_epochs: [800, 1400, 1800, 2200]
    parallel: false
    k: 5
    optimizer: 'Adam'
---
test:
    N : 6 #Number of speakers in batch
    M : 10 #Number of utterances per speaker
    num_workers: 8 #number of workers for data laoder
    epochs: 30 #testing speaker epochs
